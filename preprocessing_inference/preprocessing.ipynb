{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with the transformers tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-cased') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['check', 'out', '\\n', 'long', 'sentence', 'E.U.', '@testname']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = \"check out this long sentence E.U. @testname\".split()\n",
    "tokens[2] = '\\n'\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [\"COMUNICADO\",\"POR\",\"CORONAVIRUS\", \"\\nEl\",\"Presidente\"]  #,\"Ruso\",\"Vladimir\",\"Putin\",\"ha\",\"dicho\",\":\",\"\\\"\",\"los\",\"ciudadanos\",\"rusos\",\"tienen\",\"dos\",\"opciones\",\",\",\"se\",\"quedan\",\"en\",\"su\",\"casa\",\"por\",\"15\",\"d\\u00edas\",\"o\",\"van\",\"a\",\"prisi\\u00f3n\",\"por\",\"5\",\"a\\u00f1os\",\"\\\"\",\"FIN\",\"DEL\",\"COMUNICADO\",\".\",\" \",\"#\",\"nomequedoencasa\",\"https:\\/\\/t.co\\/zmLwunVs80\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = bert_tokenizer(tokens, return_offsets_mapping=True, is_split_into_words=True, return_special_tokens_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [4, 11498, 8664, 8375, 10772, 18100, 18169, 8895, 30980, 7078, 1162, 2503, 5], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'special_tokens_mask': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'offset_mapping': [(0, 0), (0, 5), (5, 8), (8, 10), (0, 3), (0, 3), (3, 6), (6, 8), (8, 9), (9, 11), (1, 3), (0, 10), (0, 0)]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(indices[\"offset_mapping\"][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] COMUNICADO POR CORONAVIRUS El Presidente [SEP]'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.decode(indices[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [4, 4935, 2416, 12244, 14560, 12179, 2983, 8538, 1006, 1008, 1482, 1008, 3, 13444, 21613, 30955, 5], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 3), (3, 5), (0, 3), (0, 4), (0, 4), (0, 4), (4, 8), (0, 1), (1, 2), (2, 3), (3, 4), (0, 1), (1, 5), (5, 8), (8, 9), (0, 0)]}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'COMUN',\n",
       " '##ICA',\n",
       " '##DO',\n",
       " 'POR',\n",
       " 'COR',\n",
       " '##ONA',\n",
       " '##VI',\n",
       " '##R',\n",
       " '##US',\n",
       " 'El',\n",
       " 'Presidente',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.convert_ids_to_tokens(indices[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([test,\n",
       "  \n",
       "  \n",
       "   ,\n",
       "  this],\n",
       " 3,\n",
       " '\\n\\n')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"test \\n\\n this\"\n",
    "doc = nlp(string)\n",
    "list(doc), len(doc), string[5:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.gold.biluo_tags_from_offsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacymoji\n",
    "#!pip install git+https://github.com/supadupa/spacymoji@fix-merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "import spacy\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from allennlp.data.token_indexers import PretrainedTransformerIndexer, PretrainedTransformerMismatchedIndexer, PretrainedTransformerIndexer\n",
    "from allennlp.data import Token, Vocabulary\n",
    "from biome.text.helpers import bioul_tags_to_bio_tags\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt = list(map(Path, sorted(glob.glob(\"../raw_data/subtask-2/brat/train/*.txt\"))))\n",
    "train_ann = list(map(Path, sorted(glob.glob(\"../raw_data/subtask-2/brat/train/*.ann\"))))\n",
    "\n",
    "valid_txt = list(map(Path, sorted(glob.glob(\"../raw_data/subtask-2/brat/valid//*.txt\"))))\n",
    "valid_ann = list(map(Path, sorted(glob.glob(\"../raw_data/subtask-2/brat/valid/*.ann\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = helper.get_custom_tokenizer_v1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000  ['']\n",
      "2000  ['']\n"
     ]
    }
   ],
   "source": [
    "def get_classification_dict(file_path: Path) -> Dict[str, str]:\n",
    "    \"\"\"A dict with {tweet_id: label}\"\"\"\n",
    "    classification_dict = {}\n",
    "    for i, line in enumerate(file_path.read_text().split('\\n')[1:]):\n",
    "        try:\n",
    "            tweet_id, label = line.split('\\t')\n",
    "            classification_dict[tweet_id] = label\n",
    "        except (ValueError, TypeError):\n",
    "            print(i, line, line.split('\\t'))\n",
    "    \n",
    "    return classification_dict\n",
    "\n",
    "train_classification = get_classification_dict(Path(\"../raw_data/subtask-1/train.tsv\"))\n",
    "valid_classification = get_classification_dict(Path(\"../raw_data/subtask-1/valid.tsv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(\n",
    "    txt_files: List[Path], \n",
    "    ann_files: List[Path], \n",
    "    nlp: \"spacy.nlp\", \n",
    "    classification: Dict[str, str],\n",
    "    replace_antibert_token_with: str = None,\n",
    "    bert_tokenizer: \"transformers.AutoTokenizer\" = None,\n",
    "):\n",
    "    \n",
    "    data = {\n",
    "        \"raw_text\": [],\n",
    "        \"tokens\": [],\n",
    "        \"tags_bioul\": [],\n",
    "        \"tags_bio\": [],\n",
    "        \"entity_text\": [],\n",
    "        \"classification_label\": [],\n",
    "        \"file_name\": [],\n",
    "    }\n",
    "    \n",
    "    for txt, ann in tqdm(zip(txt_files, ann_files), total=len(txt_files)):\n",
    "        try:\n",
    "            doc: spacy.docs.Doc = helper.brat2doc(\n",
    "                txt, \n",
    "                ann, \n",
    "                nlp, \n",
    "                # The competition will only evaluate PROFESION and SITUACION_LABORAL\n",
    "                ignore_labels=[\"ACTIVIDAD\", \"FIGURATIVA\"],\n",
    "                remove_children=True,\n",
    "                remove_parents=False, \n",
    "                remove_siblings=False,\n",
    "                verbose=True,\n",
    "            )\n",
    "        except ValueError as error:\n",
    "            print(txt, ann)\n",
    "            raise error\n",
    "            \n",
    "        tokens_str = list(map(str, doc))\n",
    "        if replace_antibert_token_with is not None:\n",
    "            for i, token in enumerate(tokens_str):\n",
    "                input_ids = bert_tokenizer([token], is_split_into_words=True)[\"input_ids\"]\n",
    "                if len(input_ids) <= 2:\n",
    "                    tokens_str[i] = replace_antibert_token_with\n",
    "            \n",
    "        data[\"raw_text\"].append(doc.text)\n",
    "        data[\"tokens\"].append(tokens_str)\n",
    "        data[\"tags_bioul\"].append([token._.ctag for token in doc])\n",
    "        data[\"tags_bio\"].append(bioul_tags_to_bio_tags(data[\"tags_bioul\"][-1]))\n",
    "        data[\"entity_text\"].append(doc._.entity_text)\n",
    "        data[\"classification_label\"].append(classification[txt.name.split('.')[0]])\n",
    "        data[\"file_name\"].append(txt.name)\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eddc2bade09b4d81b1fa29045509bab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Span(text='Protección Civil', label='ACTIVIDAD', file='1244001571257581568.ann')\n",
      "Removed Span(text='JUGADORES', label='ACTIVIDAD', file='1244576816133791745.ann')\n",
      "Removed Span(text='jugadores', label='ACTIVIDAD', file='1244576816133791745.ann')\n",
      "Removed Span(text='actor', label='ACTIVIDAD', file='1245657701423525888.ann')\n",
      "Removed Span(text='cantante', label='ACTIVIDAD', file='1245657701423525888.ann')\n",
      "Removed Span(text='futbolista', label='ACTIVIDAD', file='1245657701423525888.ann')\n",
      "Removed Span(text='reina', label='FIGURATIVA', file='1246395424220549120.ann')\n",
      "Removed Span(text='maestro', label='FIGURATIVA', file='1250067856190066691.ann')\n",
      "Removed Span(text='jugadores', label='ACTIVIDAD', file='1250067856190066691.ann')\n",
      "Removed Span(text='jugadores', label='ACTIVIDAD', file='1250393594046885888.ann')\n",
      "Removed Span(text='deportistas', label='ACTIVIDAD', file='1251075915943219200.ann')\n",
      "Removed Span(text='ViceKomisario', label='FIGURATIVA', file='1252685852796432384.ann')\n",
      "Removed Span(text='Amado Líder Indecente', label='FIGURATIVA', file='1252685852796432384.ann')\n",
      "Removed Span(text='twitter@s', label='ACTIVIDAD', file='1252835132983005189.ann')\n",
      "Removed Span(text='#emprendedora', label='ACTIVIDAD', file='1254840432359587840.ann')\n",
      "Removed Span(text='#emprendedoras', label='ACTIVIDAD', file='1254840432359587840.ann')\n",
      "Removed Span(text='artistas', label='ACTIVIDAD', file='1254852827559555072.ann')\n",
      "Removed Span(text='Intelectuales', label='ACTIVIDAD', file='1254852827559555072.ann')\n",
      "Removed Span(text='voluntari@s', label='ACTIVIDAD', file='1256329335734075392.ann')\n",
      "Removed Span(text='voluntarios', label='ACTIVIDAD', file='1256653622202503172.ann')\n",
      "Removed Span(text='Autora', label='FIGURATIVA', file='1256883991648616449.ann')\n",
      "Removed Span(text='activista', label='ACTIVIDAD', file='1257201616790446083.ann')\n",
      "Removed Span(text='voluntarixs', label='ACTIVIDAD', file='1259121660868857856.ann')\n",
      "Removed Span(text='monologuista', label='FIGURATIVA', file='1259808033133207553.ann')\n",
      "Removed Span(text='instagrammer', label='FIGURATIVA', file='1259808033133207553.ann')\n",
      "Removed Span(text='artistazo', label='ACTIVIDAD', file='1260251958310580231.ann')\n",
      "Removed Span(text='voluntarios', label='ACTIVIDAD', file='1260303443157286915.ann')\n",
      "Removed Span(text='voluntarios', label='ACTIVIDAD', file='1260458967962173446.ann')\n",
      "Removed Span(text='mayorales', label='FIGURATIVA', file='1261319370011262984.ann')\n",
      "Removed Span(text='artistas', label='ACTIVIDAD', file='1263524316618031105.ann')\n",
      "Removed Span(text='doctorcillo', label='FIGURATIVA', file='1263548815556182021.ann')\n",
      "Removed Span(text='corredores', label='ACTIVIDAD', file='1263766219330433026.ann')\n",
      "Removed Span(text='voluntarios', label='ACTIVIDAD', file='1263949601628925952.ann')\n",
      "Removed Span(text='consejeros', label='FIGURATIVA', file='1264853803553128450.ann')\n",
      "Removed Span(text='Guardia Civil', label='PROFESION', file='1264885356626948097.ann')\n",
      "Removed Span(text='GC', label='PROFESION', file='1265000720018870274.ann')\n",
      "Removed Span(text='guardia civil', label='PROFESION', file='1265294111688601601.ann')\n",
      "Removed Span(text='ciclistas', label='ACTIVIDAD', file='1265387509237911552.ann')\n",
      "Removed Span(text='terroristas', label='ACTIVIDAD', file='1265743912066564097.ann')\n",
      "Removed Span(text='ministro del Tiempo', label='FIGURATIVA', file='1268807584636641280.ann')\n",
      "Removed Span(text='Guardia Civil', label='PROFESION', file='1269241724645318659.ann')\n",
      "Removed Span(text='cantantes', label='ACTIVIDAD', file='1270462460496093186.ann')\n",
      "Removed Span(text='nazareno', label='ACTIVIDAD', file='1270793931207454720.ann')\n",
      "Removed Span(text='REY', label='FIGURATIVA', file='1272107543607869440.ann')\n",
      "Removed Span(text='pseudo periodistas', label='FIGURATIVA', file='1272113182937669633.ann')\n",
      "Removed Span(text='jugadores', label='ACTIVIDAD', file='1272131597370830849.ann')\n",
      "Removed Span(text='ayudantes', label='ACTIVIDAD', file='1272131597370830849.ann')\n",
      "Removed Span(text='ARTISTA', label='ACTIVIDAD', file='1272828246380490752.ann')\n",
      "Removed Span(text='cantante', label='ACTIVIDAD', file='1273361796183855111.ann')\n",
      "Removed Span(text='personas voluntarias', label='ACTIVIDAD', file='1274978290571362309.ann')\n",
      "Removed Span(text='ideólogo', label='ACTIVIDAD', file='1275000691581497345.ann')\n",
      "Removed Span(text='voluntarios', label='ACTIVIDAD', file='1275717386247770112.ann')\n",
      "Removed Span(text='voluntarios', label='ACTIVIDAD', file='1276855332984090626.ann')\n",
      "Removed Span(text='bufón', label='FIGURATIVA', file='1278610986228944897.ann')\n",
      "Removed Span(text='pedagogo', label='FIGURATIVA', file='1279658681731297280.ann')\n",
      "Removed Span(text='vocero', label='ACTIVIDAD', file='1280608237348425728.ann')\n",
      "Removed Span(text='vocera', label='ACTIVIDAD', file='1280608237348425728.ann')\n",
      "Removed Span(text='profetas', label='ACTIVIDAD', file='1281924340536287240.ann')\n",
      "Removed Span(text='miembros de las mesas', label='ACTIVIDAD', file='1282247806925840394.ann')\n",
      "Removed Span(text='reina', label='FIGURATIVA', file='1286291372627066881.ann')\n",
      "Removed Span(text='filántropos', label='ACTIVIDAD', file='1286439414785335299.ann')\n",
      "Removed Span(text='Voluntarios', label='ACTIVIDAD', file='1291102924010074112.ann')\n",
      "Removed Span(text='voluntarios', label='ACTIVIDAD', file='1291390764912717825.ann')\n",
      "Removed Span(text='voluntarios', label='ACTIVIDAD', file='1291390764912717825.ann')\n",
      "Removed Span(text='voluntarios', label='ACTIVIDAD', file='1293300892465324034.ann')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train = create_df(\n",
    "    train_txt, \n",
    "    train_ann, \n",
    "    nlp, \n",
    "    train_classification,\n",
    "    replace_antibert_token_with='æ',\n",
    "    bert_tokenizer=bert_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf8cb5a940084a6095e5f9da03afb66f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Span(text='ama de casa', label='ACTIVIDAD', file='1247422026375200769.ann')\n",
      "Removed Span(text='atleta', label='FIGURATIVA', file='1252603036524052481.ann')\n",
      "Removed Span(text='autor', label='ACTIVIDAD', file='1253703655359987712.ann')\n",
      "Removed Span(text='necropolíticos', label='FIGURATIVA', file='1255419044657758211.ann')\n",
      "Removed Span(text='necrófagos políticos', label='FIGURATIVA', file='1255419044657758211.ann')\n",
      "Removed Span(text='presidente', label='FIGURATIVA', file='1257030225277464584.ann')\n",
      "Removed Span(text='deportistas', label='ACTIVIDAD', file='1257601158065131522.ann')\n",
      "Removed Span(text='Voluntariado de Refuerzo educativo', label='ACTIVIDAD', file='1257783753277091842.ann')\n",
      "Removed Span(text='agentes', label='FIGURATIVA', file='1258327578475102209.ann')\n",
      "Removed Span(text='artistas de espectáculos públicos', label='ACTIVIDAD', file='1261015959206232072.ann')\n",
      "Removed Span(text='deportistas', label='ACTIVIDAD', file='1263780155673149442.ann')\n",
      "Removed Span(text='Autor', label='ACTIVIDAD', file='1263905083965792258.ann')\n",
      "Removed Span(text='líderes comunales', label='ACTIVIDAD', file='1266195217000140800.ann')\n",
      "Removed Span(text='hombres de trono', label='ACTIVIDAD', file='1270402085247430656.ann')\n",
      "Removed Span(text='cofrades', label='ACTIVIDAD', file='1270402085247430656.ann')\n",
      "Removed Span(text='voluntarios', label='ACTIVIDAD', file='1272166905655234565.ann')\n",
      "Removed Span(text='#ProteccionCivil', label='ACTIVIDAD', file='1272166905655234565.ann')\n",
      "Removed Span(text='tuitero', label='ACTIVIDAD', file='1272489573910818816.ann')\n",
      "Removed Span(text='dueño', label='FIGURATIVA', file='1272668645135659009.ann')\n",
      "Removed Span(text='voluntarios', label='ACTIVIDAD', file='1276105057419907073.ann')\n",
      "Removed Span(text='payasa', label='FIGURATIVA', file='1276434801206190085.ann')\n",
      "Removed Span(text='artistas', label='ACTIVIDAD', file='1291317679803052033.ann')\n",
      "Removed Span(text='Rey emérito', label='ACTIVIDAD', file='1291422649290240002.ann')\n",
      "Removed Span(text='virreyes autonómicos', label='FIGURATIVA', file='1291745879129554950.ann')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_valid = create_df(\n",
    "    valid_txt, \n",
    "    valid_ann, \n",
    "    nlp, \n",
    "    valid_classification,\n",
    "    replace_antibert_token_with='æ',\n",
    "    bert_tokenizer=bert_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cat data/profner/subtask-2/brat/train/1269241724645318659.ann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for misalignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'B-PROFESION',\n",
       "  'B-SITUACION_LABORAL',\n",
       "  'I-PROFESION',\n",
       "  'I-SITUACION_LABORAL',\n",
       "  'L-PROFESION',\n",
       "  'L-SITUACION_LABORAL',\n",
       "  'O',\n",
       "  'U-PROFESION',\n",
       "  'U-SITUACION_LABORAL'},\n",
       " {'B-PROFESION',\n",
       "  'B-SITUACION_LABORAL',\n",
       "  'I-PROFESION',\n",
       "  'I-SITUACION_LABORAL',\n",
       "  'O'},\n",
       " {'B-PROFESION',\n",
       "  'B-SITUACION_LABORAL',\n",
       "  'I-PROFESION',\n",
       "  'L-PROFESION',\n",
       "  'L-SITUACION_LABORAL',\n",
       "  'O',\n",
       "  'U-PROFESION',\n",
       "  'U-SITUACION_LABORAL'},\n",
       " {'B-PROFESION',\n",
       "  'B-SITUACION_LABORAL',\n",
       "  'I-PROFESION',\n",
       "  'I-SITUACION_LABORAL',\n",
       "  'O'})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_train.tags_bioul.sum()), set(df_train.tags_bio.sum()), set(df_valid.tags_bioul.sum()), set(df_valid.tags_bio.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df_valid.itertuples():\n",
    "    if '-' in row.tags_bioul:\n",
    "        print(row.file_name)\n",
    "        print(list(zip(row.tokens, row.tags_bioul)), row.raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags_bioul</th>\n",
       "      <th>tags_bio</th>\n",
       "      <th>entity_text</th>\n",
       "      <th>classification_label</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cerramos nuestra querida Radio 😢 Nuestros cola...</td>\n",
       "      <td>[Cerramos, nuestra, querida, Radio, 😢, Nuestro...</td>\n",
       "      <td>[O, O, O, O, O, O, U-PROFESION, O, U-PROFESION...</td>\n",
       "      <td>[O, O, O, O, O, O, B-PROFESION, O, B-PROFESION...</td>\n",
       "      <td>[colaboradores, conductores]</td>\n",
       "      <td>1</td>\n",
       "      <td>1242399976644325376.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#OtroEscandalo #HastaCuando \\n#DenunciaCCOO #C...</td>\n",
       "      <td>[#, OtroEscandalo, #, HastaCuando, \\n, #, Denu...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1242406334802395137.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>¿Es necesario entregar nuestra privacidad a un...</td>\n",
       "      <td>[¿, Es, necesario, entregar, nuestra, privacid...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1242407077278093313.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Así que estás chimbeando mucho con esos Decret...</td>\n",
       "      <td>[Así, que, estás, chimbeando, mucho, con, esos...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, U-PROFESION,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-PROFESION,...</td>\n",
       "      <td>[Presidente]</td>\n",
       "      <td>1</td>\n",
       "      <td>1242407274771030016.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@FeGarPe79 @escipion_r @LuciaMendezEM Estás MU...</td>\n",
       "      <td>[@FeGarPe79, @escipion_r, @LuciaMendezEM, Está...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1242409866515435520.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>Se avecina un nuevo confinamiento q es una man...</td>\n",
       "      <td>[Se, avecina, un, nuevo, confinamiento, q, es,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1293639784397766656.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>Así funciona Radar COVID, la app de rastreo de...</td>\n",
       "      <td>[Así, funciona, Radar, COVID, ,, la, app, de, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1293642161867632641.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>Se duplican los contagios por Coronavirus en M...</td>\n",
       "      <td>[Se, duplican, los, contagios, por, Coronaviru...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1293651264140726272.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>Corríjanme si me equivoco, pero somos el único...</td>\n",
       "      <td>[Corríjanme, si, me, equivoco, ,, pero, somos,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1293654036722442247.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>Así ha colaborado el hombre sin saberlo con el...</td>\n",
       "      <td>[Así, ha, colaborado, el, hombre, sin, saberlo...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1293654313311637506.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               raw_text  \\\n",
       "0     Cerramos nuestra querida Radio 😢 Nuestros cola...   \n",
       "1     #OtroEscandalo #HastaCuando \\n#DenunciaCCOO #C...   \n",
       "2     ¿Es necesario entregar nuestra privacidad a un...   \n",
       "3     Así que estás chimbeando mucho con esos Decret...   \n",
       "4     @FeGarPe79 @escipion_r @LuciaMendezEM Estás MU...   \n",
       "...                                                 ...   \n",
       "5995  Se avecina un nuevo confinamiento q es una man...   \n",
       "5996  Así funciona Radar COVID, la app de rastreo de...   \n",
       "5997  Se duplican los contagios por Coronavirus en M...   \n",
       "5998  Corríjanme si me equivoco, pero somos el único...   \n",
       "5999  Así ha colaborado el hombre sin saberlo con el...   \n",
       "\n",
       "                                                 tokens  \\\n",
       "0     [Cerramos, nuestra, querida, Radio, 😢, Nuestro...   \n",
       "1     [#, OtroEscandalo, #, HastaCuando, \\n, #, Denu...   \n",
       "2     [¿, Es, necesario, entregar, nuestra, privacid...   \n",
       "3     [Así, que, estás, chimbeando, mucho, con, esos...   \n",
       "4     [@FeGarPe79, @escipion_r, @LuciaMendezEM, Está...   \n",
       "...                                                 ...   \n",
       "5995  [Se, avecina, un, nuevo, confinamiento, q, es,...   \n",
       "5996  [Así, funciona, Radar, COVID, ,, la, app, de, ...   \n",
       "5997  [Se, duplican, los, contagios, por, Coronaviru...   \n",
       "5998  [Corríjanme, si, me, equivoco, ,, pero, somos,...   \n",
       "5999  [Así, ha, colaborado, el, hombre, sin, saberlo...   \n",
       "\n",
       "                                             tags_bioul  \\\n",
       "0     [O, O, O, O, O, O, U-PROFESION, O, U-PROFESION...   \n",
       "1     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3     [O, O, O, O, O, O, O, O, O, O, O, U-PROFESION,...   \n",
       "4     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "...                                                 ...   \n",
       "5995  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "5996  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "5997  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "5998  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "5999  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                               tags_bio  \\\n",
       "0     [O, O, O, O, O, O, B-PROFESION, O, B-PROFESION...   \n",
       "1     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3     [O, O, O, O, O, O, O, O, O, O, O, B-PROFESION,...   \n",
       "4     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "...                                                 ...   \n",
       "5995  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "5996  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "5997  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "5998  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "5999  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                       entity_text classification_label  \\\n",
       "0     [colaboradores, conductores]                    1   \n",
       "1                               []                    0   \n",
       "2                               []                    0   \n",
       "3                     [Presidente]                    1   \n",
       "4                               []                    0   \n",
       "...                            ...                  ...   \n",
       "5995                            []                    0   \n",
       "5996                            []                    0   \n",
       "5997                            []                    0   \n",
       "5998                            []                    0   \n",
       "5999                            []                    0   \n",
       "\n",
       "                    file_name  \n",
       "0     1242399976644325376.txt  \n",
       "1     1242406334802395137.txt  \n",
       "2     1242407077278093313.txt  \n",
       "3     1242407274771030016.txt  \n",
       "4     1242409866515435520.txt  \n",
       "...                       ...  \n",
       "5995  1293639784397766656.txt  \n",
       "5996  1293642161867632641.txt  \n",
       "5997  1293651264140726272.txt  \n",
       "5998  1293654036722442247.txt  \n",
       "5999  1293654313311637506.txt  \n",
       "\n",
       "[6000 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags_bioul</th>\n",
       "      <th>tags_bio</th>\n",
       "      <th>entity_text</th>\n",
       "      <th>classification_label</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COMUNICADO POR CORONAVIRUS \\nEl Presidente Rus...</td>\n",
       "      <td>[COMUNICADO, POR, CORONAVIRUS, æ, El, Presiden...</td>\n",
       "      <td>[O, O, O, O, O, U-PROFESION, O, O, O, O, O, O,...</td>\n",
       "      <td>[O, O, O, O, O, B-PROFESION, O, O, O, O, O, O,...</td>\n",
       "      <td>[Presidente]</td>\n",
       "      <td>1</td>\n",
       "      <td>1242407018465579008.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“La falta de transparencia en asuntos de salud...</td>\n",
       "      <td>[“, La, falta, de, transparencia, en, asuntos,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1242486580222103554.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Las enseñanzas del coronavirus |\\nPero @jcoscu...</td>\n",
       "      <td>[Las, enseñanzas, del, coronavirus, |, æ, Pero...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1242506188555718656.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No me alegro de la muerte de nadie, ¿pero es m...</td>\n",
       "      <td>[No, me, alegro, de, la, muerte, de, nadie, ,,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[guardias civiles]</td>\n",
       "      <td>1</td>\n",
       "      <td>1242686975943094273.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UNIDOS venceremos al Coronavirus #COVID19 http...</td>\n",
       "      <td>[UNIDOS, venceremos, al, Coronavirus, #, COVID...</td>\n",
       "      <td>[O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1242726918132301825.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>—Me he bajado la app esa para detectar enfermo...</td>\n",
       "      <td>[—, Me, he, bajado, la, app, esa, para, detect...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1293545412066975744.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>Desde el 7 de marzo, y mascarilla en ristre, a...</td>\n",
       "      <td>[Desde, el, 7, de, marzo, ,, y, mascarilla, en...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1293561267601510402.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>Sigan para bingo en este 2020</td>\n",
       "      <td>[Sigan, para, bingo, en, este, 2020]</td>\n",
       "      <td>[O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1293579520000368640.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>🇪🇸 | URGENTE - CORONAVIRUS: España reporta 169...</td>\n",
       "      <td>[🇪, 🇸, |, URGENTE, -, CORONAVIRUS, :, España, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1293598083545214980.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>La mascarilla, en días de calor, puede ser alg...</td>\n",
       "      <td>[La, mascarilla, ,, en, días, de, calor, ,, pu...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1293647654359117827.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               raw_text  \\\n",
       "0     COMUNICADO POR CORONAVIRUS \\nEl Presidente Rus...   \n",
       "1     “La falta de transparencia en asuntos de salud...   \n",
       "2     Las enseñanzas del coronavirus |\\nPero @jcoscu...   \n",
       "3     No me alegro de la muerte de nadie, ¿pero es m...   \n",
       "4     UNIDOS venceremos al Coronavirus #COVID19 http...   \n",
       "...                                                 ...   \n",
       "1995  —Me he bajado la app esa para detectar enfermo...   \n",
       "1996  Desde el 7 de marzo, y mascarilla en ristre, a...   \n",
       "1997                      Sigan para bingo en este 2020   \n",
       "1998  🇪🇸 | URGENTE - CORONAVIRUS: España reporta 169...   \n",
       "1999  La mascarilla, en días de calor, puede ser alg...   \n",
       "\n",
       "                                                 tokens  \\\n",
       "0     [COMUNICADO, POR, CORONAVIRUS, æ, El, Presiden...   \n",
       "1     [“, La, falta, de, transparencia, en, asuntos,...   \n",
       "2     [Las, enseñanzas, del, coronavirus, |, æ, Pero...   \n",
       "3     [No, me, alegro, de, la, muerte, de, nadie, ,,...   \n",
       "4     [UNIDOS, venceremos, al, Coronavirus, #, COVID...   \n",
       "...                                                 ...   \n",
       "1995  [—, Me, he, bajado, la, app, esa, para, detect...   \n",
       "1996  [Desde, el, 7, de, marzo, ,, y, mascarilla, en...   \n",
       "1997               [Sigan, para, bingo, en, este, 2020]   \n",
       "1998  [🇪, 🇸, |, URGENTE, -, CORONAVIRUS, :, España, ...   \n",
       "1999  [La, mascarilla, ,, en, días, de, calor, ,, pu...   \n",
       "\n",
       "                                             tags_bioul  \\\n",
       "0     [O, O, O, O, O, U-PROFESION, O, O, O, O, O, O,...   \n",
       "1     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "4                                 [O, O, O, O, O, O, O]   \n",
       "...                                                 ...   \n",
       "1995  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1996  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1997                                 [O, O, O, O, O, O]   \n",
       "1998  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1999  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                               tags_bio         entity_text  \\\n",
       "0     [O, O, O, O, O, B-PROFESION, O, O, O, O, O, O,...        [Presidente]   \n",
       "1     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...                  []   \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...                  []   \n",
       "3     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  [guardias civiles]   \n",
       "4                                 [O, O, O, O, O, O, O]                  []   \n",
       "...                                                 ...                 ...   \n",
       "1995  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...                  []   \n",
       "1996  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...                  []   \n",
       "1997                                 [O, O, O, O, O, O]                  []   \n",
       "1998  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...                  []   \n",
       "1999  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...                  []   \n",
       "\n",
       "     classification_label                file_name  \n",
       "0                       1  1242407018465579008.txt  \n",
       "1                       0  1242486580222103554.txt  \n",
       "2                       0  1242506188555718656.txt  \n",
       "3                       1  1242686975943094273.txt  \n",
       "4                       0  1242726918132301825.txt  \n",
       "...                   ...                      ...  \n",
       "1995                    0  1293545412066975744.txt  \n",
       "1996                    0  1293561267601510402.txt  \n",
       "1997                    0  1293579520000368640.txt  \n",
       "1998                    0  1293598083545214980.txt  \n",
       "1999                    0  1293647654359117827.txt  \n",
       "\n",
       "[2000 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_json(\"train_v2.json\", lines=True, orient=\"records\")\n",
    "df_valid.to_json(\"valid_v2.json\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing with the mask concept\n",
    "Basically only tag the first transformer token of a spacy token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields import TextField\n",
    "from allennlp.data import Token\n",
    "from allennlp.data.token_indexers import PretrainedTransformerIndexer\n",
    "from allennlp.data import Batch\n",
    "from allennlp.data import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = pd.read_json(\"valid_v1.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"dccuchile/bert-base-spanish-wwm-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(df_valid.tokens[0], is_split_into_words=True, return_offsets_mapping=True)\n",
    "allennlp_tokens = list(map(Token, tokenizer.convert_ids_to_tokens(input_ids[\"input_ids\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = PretrainedTransformerIndexer(model_name=model_name)\n",
    "text_field = TextField(allennlp_tokens, token_indexers={\"transformers\": indexer})\n",
    "instance = Instance({\"tokens\": text_field})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a19608c1c84213a36a40d753de5118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='building vocab', max=1.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.from_instances([instance])\n",
    "batch = Batch([instance])\n",
    "batch.index_instances(vocab)\n",
    "token_ids = batch.as_tensor_dict()[\"tokens\"][\"transformers\"]\n",
    "indices = indexer.tokens_to_indices(allennlp_tokens, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_offsets(df, model_name=\"dccuchile/bert-base-spanish-wwm-uncased\") -> Tuple[List, List]:\n",
    "    indexer = PretrainedTransformerIndexer(model_name=model_name)\n",
    "    vocab = Vocabulary()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name) \n",
    "    \n",
    "    for row in tqdm(df.itertuples(), total=len(df)):\n",
    "        indices = tokenizer(row.tokens, return_offsets_mapping=True, is_split_into_words=True, return_special_tokens_mask=True)\n",
    "        print\n",
    "        token_groups = []\n",
    "        for input_id, offset in zip(indices[\"input_ids\"][1:-1], indices[\"offset_mapping\"][1:-1]):\n",
    "            if offset[0] == 0:\n",
    "                token_groups.append([tokenizer.convert_ids_to_tokens(input_id)])\n",
    "            else:\n",
    "                token_groups[-1].append(tokenizer.convert_ids_to_tokens(input_id))\n",
    "        print(list(zip(row.tokens, token_groups)))\n",
    "        tokens_str = tokenizer.convert_ids_to_tokens(indices[\"input_ids\"])\n",
    "        tokens = [Token(tok) for tok in tokens_str]\n",
    "        token_indexes = indexer.tokens_to_indices(tokens, vocabulary=vocab)\n",
    "        token_ids = token_indexes[\"token_ids\"]\n",
    "                    \n",
    "    return destroyed_tokens, destroyed_tags, offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf2bd91dc4347a29ab43950a2e07dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('COMUNICADO', ['comunicado']), ('POR', ['por']), ('CORONAVIRUS', ['corona', '##vir', '##us']), ('\\n', ['el']), ('El', ['presidente']), ('Presidente', ['ruso']), ('Ruso', ['vladimir']), ('Vladimir', ['pu', '##tin']), ('Putin', ['ha']), ('ha', ['dicho']), ('dicho', [':']), (':', ['\"']), ('\"', ['los']), ('los', ['ciudadanos']), ('ciudadanos', ['rusos']), ('rusos', ['tienen']), ('tienen', ['dos']), ('dos', ['opciones']), ('opciones', [',']), (',', ['se']), ('se', ['quedan']), ('quedan', ['en']), ('en', ['su']), ('su', ['casa']), ('casa', ['por']), ('por', ['15']), ('15', ['dias']), ('días', ['o']), ('o', ['van']), ('van', ['a']), ('a', ['prision']), ('prisión', ['por']), ('por', ['5']), ('5', ['anos']), ('años', ['\"']), ('\"', ['fin']), ('FIN', ['del']), ('DEL', ['comunicado']), ('COMUNICADO', ['.']), ('.', ['[UNK]']), (' ', ['nom', '##e', '##que', '##do', '##en', '##cas', '##a']), ('#', ['h', '##tt', '##ps', ':', '[UNK]', '[UNK]', 't', '.', 'co', '[UNK]', 'z', '##ml', '##w', '##un', '##v', '##s', '##8', '##0'])]\n",
      "[('“', ['[UNK]']), ('La', ['la']), ('falta', ['falta']), ('de', ['de']), ('transparencia', ['transparencia']), ('en', ['en']), ('asuntos', ['asuntos']), ('de', ['de']), ('salud', ['salud']), ('pública', ['publica']), ('tendría', ['tendr', '##ia']), ('que', ['que']), ('ser', ['ser']), ('un', ['un']), ('crimen', ['crimen']), ('de', ['de']), ('lesa', ['lesa']), ('humanidad', ['humanidad']), ('.', ['.']), (' ', ['esto']), ('Esto', ['nos']), ('nos', ['demuestra']), ('demuestra', ['que']), ('que', ['los']), ('los', ['reg', '##imen', '##es']), ('regímenes', ['autor', '##itarios']), ('autoritarios', ['y']), ('y', ['herm', '##et', '##icos']), ('herméticos', ['son']), ('son', ['un']), ('un', ['peligro']), ('peligro', ['para']), ('para', ['la']), ('la', ['salud']), ('salud', ['y']), ('y', ['la']), ('la', ['paz']), ('paz', ['mundial']), ('mundial', ['[UNK]']), ('”', ['.', '.', '.'])]\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'destroyed_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-2a1a5d2aa9d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcheck_offsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-49-84bda4d38a30>\u001b[0m in \u001b[0;36mcheck_offsets\u001b[0;34m(df, model_name)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_indexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"token_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdestroyed_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestroyed_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'destroyed_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "check_offsets(df_valid[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biome.text import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset json (/home/david/.cache/huggingface/datasets/json/default-b2dd006bd782a885/0.0.0/70d89ed4db1394f028c651589fcab6d6b28dddcabbe39d3b21b4d41f9a708514)\n"
     ]
    }
   ],
   "source": [
    "ds = Dataset.from_json(\"valid_v1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = tokenizer(ds[0][\"tokens\"], return_offsets_mapping=True, is_split_into_words=True, return_special_tokens_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "for off in indices[\"offset_mapping\"]:\n",
    "    if off[0] == 0:\n",
    "        i += 1\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds[0][\"tags_bio\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training.metrics import SpanBasedF1Measure\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = SpanBasedF1Measure(Vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [[\"B-TEST\", \"I-TEST\", \"O\"], ...]\n",
    "gold = [\"B-TEST\", \"O\", \"O\"]\n",
    "vocab = {\"B-TEST\": 0, \"I-TEST\": 1, \"O\": 2}\n",
    "\n",
    "vocab = {\"B-8474693\", \"I-...\", ...........}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m     \n",
       "\u001b[0mf1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpredictions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgold_labels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBoolTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprediction_map\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mType:\u001b[0m           SpanBasedF1Measure\n",
       "\u001b[0;31mString form:\u001b[0m    <allennlp.training.metrics.span_based_f1_measure.SpanBasedF1Measure object at 0x7f712113e6d0>\n",
       "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/biome/lib/python3.7/site-packages/allennlp/training/metrics/span_based_f1_measure.py\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "The Conll SRL metrics are based on exact span matching. This metric\n",
       "implements span-based precision and recall metrics for a BIO tagging\n",
       "scheme. It will produce precision, recall and F1 measures per tag, as\n",
       "well as overall statistics. Note that the implementation of this metric\n",
       "is not exactly the same as the perl script used to evaluate the CONLL 2005\n",
       "data - particularly, it does not consider continuations or reference spans\n",
       "as constituents of the original span. However, it is a close proxy, which\n",
       "can be helpful for judging model performance during training. This metric\n",
       "works properly when the spans are unlabeled (i.e., your labels are\n",
       "simply \"B\", \"I\", \"O\" if using the \"BIO\" label encoding).\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "# Parameters\n",
       "\n",
       "vocabulary : `Vocabulary`, required.\n",
       "    A vocabulary containing the tag namespace.\n",
       "\n",
       "tag_namespace : `str`, required.\n",
       "    This metric assumes that a BIO format is used in which the\n",
       "    labels are of the format: [\"B-LABEL\", \"I-LABEL\"].\n",
       "\n",
       "ignore_classes : `List[str]`, optional.\n",
       "    Span labels which will be ignored when computing span metrics.\n",
       "    A \"span label\" is the part that comes after the BIO label, so it\n",
       "    would be \"ARG1\" for the tag \"B-ARG1\". For example by passing:\n",
       "\n",
       "     `ignore_classes=[\"V\"]`\n",
       "    the following sequence would not consider the \"V\" span at index (2, 3)\n",
       "    when computing the precision, recall and F1 metrics.\n",
       "\n",
       "    [\"O\", \"O\", \"B-V\", \"I-V\", \"B-ARG1\", \"I-ARG1\"]\n",
       "\n",
       "    This is helpful for instance, to avoid computing metrics for \"V\"\n",
       "    spans in a BIO tagging scheme which are typically not included.\n",
       "\n",
       "label_encoding : `str`, optional (default = `\"BIO\"`)\n",
       "    The encoding used to specify label span endpoints in the sequence.\n",
       "    Valid options are \"BIO\", \"IOB1\", \"BIOUL\" or \"BMES\".\n",
       "\n",
       "tags_to_spans_function : `Callable`, optional (default = `None`)\n",
       "    If `label_encoding` is `None`, `tags_to_spans_function` will be\n",
       "    used to generate spans.\n",
       "\u001b[0;31mCall docstring:\u001b[0m\n",
       "# Parameters\n",
       "\n",
       "predictions : `torch.Tensor`, required.\n",
       "    A tensor of predictions of shape (batch_size, sequence_length, num_classes).\n",
       "gold_labels : `torch.Tensor`, required.\n",
       "    A tensor of integer class label of shape (batch_size, sequence_length). It must be the same\n",
       "    shape as the `predictions` tensor without the `num_classes` dimension.\n",
       "mask : `torch.BoolTensor`, optional (default = `None`).\n",
       "    A masking tensor the same size as `gold_labels`.\n",
       "prediction_map : `torch.Tensor`, optional (default = `None`).\n",
       "    A tensor of size (batch_size, num_classes) which provides a mapping from the index of predictions\n",
       "    to the indices of the label vocabulary. If provided, the output label at each timestep will be\n",
       "    `vocabulary.get_index_to_token_vocabulary(prediction_map[batch, argmax(predictions[batch, t]))`,\n",
       "    rather than simply `vocabulary.get_index_to_token_vocabulary(argmax(predictions[batch, t]))`.\n",
       "    This is useful in cases where each Instance in the dataset is associated with a different possible\n",
       "    subset of labels from a large label-space (IE FrameNet, where each frame has a different set of\n",
       "    possible roles associated with it).\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = torch.tensor([[[1, 0, 0], [0, 1, 0], [0, 0, 1]], [...], [...]]) \n",
    "gold = torch.tensor([[[1, 0, 0], [0, 0, 1], [0, 0, 1]]])\n",
    "\n",
    "len(predictions.size()) == 3\n",
    "# batchsize, nr of words, nr of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m     \n",
       "\u001b[0mf1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpredictions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgold_labels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBoolTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprediction_map\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mType:\u001b[0m           SpanBasedF1Measure\n",
       "\u001b[0;31mString form:\u001b[0m    <allennlp.training.metrics.span_based_f1_measure.SpanBasedF1Measure object at 0x7f7121209e90>\n",
       "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/biome/lib/python3.7/site-packages/allennlp/training/metrics/span_based_f1_measure.py\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "The Conll SRL metrics are based on exact span matching. This metric\n",
       "implements span-based precision and recall metrics for a BIO tagging\n",
       "scheme. It will produce precision, recall and F1 measures per tag, as\n",
       "well as overall statistics. Note that the implementation of this metric\n",
       "is not exactly the same as the perl script used to evaluate the CONLL 2005\n",
       "data - particularly, it does not consider continuations or reference spans\n",
       "as constituents of the original span. However, it is a close proxy, which\n",
       "can be helpful for judging model performance during training. This metric\n",
       "works properly when the spans are unlabeled (i.e., your labels are\n",
       "simply \"B\", \"I\", \"O\" if using the \"BIO\" label encoding).\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "# Parameters\n",
       "\n",
       "vocabulary : `Vocabulary`, required.\n",
       "    A vocabulary containing the tag namespace.\n",
       "\n",
       "tag_namespace : `str`, required.\n",
       "    This metric assumes that a BIO format is used in which the\n",
       "    labels are of the format: [\"B-LABEL\", \"I-LABEL\"].\n",
       "\n",
       "ignore_classes : `List[str]`, optional.\n",
       "    Span labels which will be ignored when computing span metrics.\n",
       "    A \"span label\" is the part that comes after the BIO label, so it\n",
       "    would be \"ARG1\" for the tag \"B-ARG1\". For example by passing:\n",
       "\n",
       "     `ignore_classes=[\"V\"]`\n",
       "    the following sequence would not consider the \"V\" span at index (2, 3)\n",
       "    when computing the precision, recall and F1 metrics.\n",
       "\n",
       "    [\"O\", \"O\", \"B-V\", \"I-V\", \"B-ARG1\", \"I-ARG1\"]\n",
       "\n",
       "    This is helpful for instance, to avoid computing metrics for \"V\"\n",
       "    spans in a BIO tagging scheme which are typically not included.\n",
       "\n",
       "label_encoding : `str`, optional (default = `\"BIO\"`)\n",
       "    The encoding used to specify label span endpoints in the sequence.\n",
       "    Valid options are \"BIO\", \"IOB1\", \"BIOUL\" or \"BMES\".\n",
       "\n",
       "tags_to_spans_function : `Callable`, optional (default = `None`)\n",
       "    If `label_encoding` is `None`, `tags_to_spans_function` will be\n",
       "    used to generate spans.\n",
       "\u001b[0;31mCall docstring:\u001b[0m\n",
       "# Parameters\n",
       "\n",
       "predictions : `torch.Tensor`, required.\n",
       "    A tensor of predictions of shape (batch_size, sequence_length, num_classes).\n",
       "gold_labels : `torch.Tensor`, required.\n",
       "    A tensor of integer class label of shape (batch_size, sequence_length). It must be the same\n",
       "    shape as the `predictions` tensor without the `num_classes` dimension.\n",
       "mask : `torch.BoolTensor`, optional (default = `None`).\n",
       "    A masking tensor the same size as `gold_labels`.\n",
       "prediction_map : `torch.Tensor`, optional (default = `None`).\n",
       "    A tensor of size (batch_size, num_classes) which provides a mapping from the index of predictions\n",
       "    to the indices of the label vocabulary. If provided, the output label at each timestep will be\n",
       "    `vocabulary.get_index_to_token_vocabulary(prediction_map[batch, argmax(predictions[batch, t]))`,\n",
       "    rather than simply `vocabulary.get_index_to_token_vocabulary(argmax(predictions[batch, t]))`.\n",
       "    This is useful in cases where each Instance in the dataset is associated with a different possible\n",
       "    subset of labels from a large label-space (IE FrameNet, where each frame has a different set of\n",
       "    possible roles associated with it).\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f1(predictions, gold, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
