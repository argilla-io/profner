{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "This notebook produces the output required for a submission to the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biome.text import Pipeline, Dataset\n",
    "from typing import List, Dict\n",
    "from helper import get_custom_tokenizer_v1\n",
    "from spacy.gold import offsets_from_biluo_tags\n",
    "from allennlp.data.dataset_readers.dataset_utils.span_utils import to_bioul\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import spacy\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../experiments/final_transformer_model/model.tar.gz\"\n",
    "#model_path = \"../experiments/final_rnn_model/model.tar.gz\"\n",
    "\n",
    "pipeline = Pipeline.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformer model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110446890"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.num_trainable_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.5 ms ± 854 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "pipeline.predict([\"test\", \"this\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RNN model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15187102"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.num_trainable_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7 ms ± 103 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "pipeline.predict([\"test\", \"this\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.272413789016496, 6.621621621621621)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "110446890 / 15187102, 24.5 / 3.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess input data\n",
    "\n",
    "Apply the same preprocessing as we did for the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = get_custom_tokenizer_v1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-cased') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_files = list(map(Path, sorted(glob.glob(\"../raw_data/profner_test+background/subtask-1/test-background-txt-files/*.txt\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference_dataset(\n",
    "    txt_files: List[Path], \n",
    "    nlp: spacy.language.Language, \n",
    "    replace_antibert_token_with: str = None, \n",
    "    bert_tokenizer: \"transformers.AutoTokenizer\" = None\n",
    "):\n",
    "    data = {\n",
    "        \"raw_text\": [],\n",
    "        \"tokens\": [],\n",
    "        \"file_name\": [],\n",
    "    }\n",
    "    \n",
    "    for txt in tqdm(txt_files, total=len(txt_files)):\n",
    "        doc = nlp(txt.read_text())\n",
    "\n",
    "        tokens_str = list(map(str, doc))\n",
    "        if replace_antibert_token_with is not None:\n",
    "            for i, token in enumerate(tokens_str):\n",
    "                input_ids = bert_tokenizer([token], is_split_into_words=True)[\"input_ids\"]\n",
    "                if len(input_ids) <= 2:\n",
    "                    tokens_str[i] = replace_antibert_token_with\n",
    "\n",
    "        data[\"raw_text\"].append(doc.text)\n",
    "        data[\"tokens\"].append(tokens_str)\n",
    "        data[\"file_name\"].append(txt.name)\n",
    "        \n",
    "    return Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe953b0a9cc4663851170b7809b0634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=27000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = get_inference_dataset(\n",
    "    txt_files,\n",
    "    nlp,\n",
    "    replace_antibert_token_with=\"æ\",\n",
    "    bert_tokenizer=bert_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c85ac08a5446e9974ac55d50580f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def batch_prediction(tokens_list):\n",
    "    batch = [{\"tokens\": tokens} for tokens in tokens_list]\n",
    "    return {\"predictions\": pipeline.predict(batch=batch)}\n",
    "    \n",
    "dataset = dataset.map(batch_prediction, input_columns=\"tokens\", batched=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8e12f4a62c4b3b81e78fae2a41e4cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def batch_classification_output(file_names: List[str], predictions: List[Dict]):\n",
    "    return {\n",
    "        \"tweet_id\": [file_name.split('.')[0] for file_name in file_names],\n",
    "        \"label\": [prediction[\"classification_labels\"][0] for prediction in predictions],                 \n",
    "    }\n",
    "    \n",
    "ds = dataset.map(\n",
    "    batch_classification_output,\n",
    "    input_columns=[\"file_name\", \"predictions\"],\n",
    "    batched=True,\n",
    "    batch_size=2,\n",
    "    remove_columns=dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1242399368143147012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1242402119623286791</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1242402388574601216</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1242402392475340803</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1242402598642167808</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26995</th>\n",
       "      <td>1293657766507274245</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26996</th>\n",
       "      <td>1293659187654909958</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26997</th>\n",
       "      <td>1293661906151051264</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26998</th>\n",
       "      <td>1293662952831188992</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26999</th>\n",
       "      <td>1293668015725314049</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tweet_id label\n",
       "0      1242399368143147012     0\n",
       "1      1242402119623286791     0\n",
       "2      1242402388574601216     1\n",
       "3      1242402392475340803     0\n",
       "4      1242402598642167808     0\n",
       "...                    ...   ...\n",
       "26995  1293657766507274245     0\n",
       "26996  1293659187654909958     0\n",
       "26997  1293661906151051264     0\n",
       "26998  1293662952831188992     0\n",
       "26999  1293668015725314049     0\n",
       "\n",
       "[27000 rows x 2 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = ds.head(n=None)[[\"tweet_id\", \"label\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"submissions/final_transformer_model/task7a_test.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = get_custom_tokenizer_v1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_ner_output(file_names: List[str], raw_text: List[str], predictions: List[Dict]):\n",
    "    #print(file_names, raw_text, predictions)\n",
    "    docs = [nlp(text) for text in raw_text]\n",
    "    bioul_tags = [to_bioul(prediction[\"ner_tags\"]) for prediction in predictions]\n",
    "    batch_offsets = [offsets_from_biluo_tags(doc, entities) for doc, entities in zip(docs, bioul_tags)]\n",
    "    tweet_ids = [file_name.split('.')[0] for file_name in file_names]\n",
    "    #print(tweet_ids, bioul_tags, batch_offsets, raw_text)\n",
    "    \n",
    "    tweet_id, begin, end, ent_type, extraction = [], [], [], [], []\n",
    "    for tid, offsets, text in zip(tweet_ids, batch_offsets, raw_text):\n",
    "        tweet_id += [tid]*len(offsets)\n",
    "        begin += [offset[0] for offset in offsets]\n",
    "        end += [offset[1] for offset in offsets]\n",
    "        ent_type += [offset[2] for offset in offsets]\n",
    "        extraction += [text[offset[0]:offset[1]] for offset in offsets]\n",
    "    \n",
    "    if any(['\\n' in ext for ext in extraction]):\n",
    "        print(\"Found 'newline' in extraction!!\")\n",
    "    #print(tweet_id, begin, end, ent_type, extraction)\n",
    "    return {\n",
    "        \"tweet_id\": tweet_id,\n",
    "        \"begin\":begin,\n",
    "        \"end\": end,\n",
    "        \"type\":ent_type,\n",
    "        \"extraction\": extraction,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128d7765e01f441bb679cd52535a116d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3375.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 'newline' in extraction!!\n",
      "Found 'newline' in extraction!!\n",
      "Found 'newline' in extraction!!\n",
      "Found 'newline' in extraction!!\n",
      "Found 'newline' in extraction!!\n",
      "Found 'newline' in extraction!!\n",
      "Found 'newline' in extraction!!\n",
      "Found 'newline' in extraction!!\n",
      "Found 'newline' in extraction!!\n",
      "Found 'newline' in extraction!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds = dataset.map(\n",
    "    batch_ner_output,\n",
    "    input_columns=[\"file_name\", \"raw_text\", \"predictions\"],\n",
    "    batched=True,\n",
    "    batch_size=8,\n",
    "    remove_columns=dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>type</th>\n",
       "      <th>extraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1242402388574601216</td>\n",
       "      <td>63</td>\n",
       "      <td>71</td>\n",
       "      <td>SITUACION_LABORAL</td>\n",
       "      <td>personal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1242409027801419777</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>SITUACION_LABORAL</td>\n",
       "      <td>universitarios</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1242409027801419777</td>\n",
       "      <td>108</td>\n",
       "      <td>119</td>\n",
       "      <td>PROFESION</td>\n",
       "      <td>#sanitarios</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1242420268397662208</td>\n",
       "      <td>22</td>\n",
       "      <td>32</td>\n",
       "      <td>PROFESION</td>\n",
       "      <td>sanitarios</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1242437532123611136</td>\n",
       "      <td>24</td>\n",
       "      <td>60</td>\n",
       "      <td>PROFESION</td>\n",
       "      <td>personal contratado de enfermería de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7256</th>\n",
       "      <td>1293537468730552322</td>\n",
       "      <td>139</td>\n",
       "      <td>151</td>\n",
       "      <td>SITUACION_LABORAL</td>\n",
       "      <td>trabajadoras</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7257</th>\n",
       "      <td>1293537468730552322</td>\n",
       "      <td>154</td>\n",
       "      <td>166</td>\n",
       "      <td>SITUACION_LABORAL</td>\n",
       "      <td>trabajadores</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7258</th>\n",
       "      <td>1293575377323995138</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>PROFESION</td>\n",
       "      <td>funcionarios de la prisión</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7259</th>\n",
       "      <td>1293599239046651911</td>\n",
       "      <td>98</td>\n",
       "      <td>108</td>\n",
       "      <td>PROFESION</td>\n",
       "      <td>presidente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7260</th>\n",
       "      <td>1293656783349784576</td>\n",
       "      <td>44</td>\n",
       "      <td>49</td>\n",
       "      <td>SITUACION_LABORAL</td>\n",
       "      <td>preso</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7261 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id  begin  end               type  \\\n",
       "0     1242402388574601216     63   71  SITUACION_LABORAL   \n",
       "1     1242409027801419777      4   18  SITUACION_LABORAL   \n",
       "2     1242409027801419777    108  119          PROFESION   \n",
       "3     1242420268397662208     22   32          PROFESION   \n",
       "4     1242437532123611136     24   60          PROFESION   \n",
       "...                   ...    ...  ...                ...   \n",
       "7256  1293537468730552322    139  151  SITUACION_LABORAL   \n",
       "7257  1293537468730552322    154  166  SITUACION_LABORAL   \n",
       "7258  1293575377323995138      4   30          PROFESION   \n",
       "7259  1293599239046651911     98  108          PROFESION   \n",
       "7260  1293656783349784576     44   49  SITUACION_LABORAL   \n",
       "\n",
       "                                extraction  \n",
       "0                                 personal  \n",
       "1                           universitarios  \n",
       "2                              #sanitarios  \n",
       "3                               sanitarios  \n",
       "4     personal contratado de enfermería de  \n",
       "...                                    ...  \n",
       "7256                          trabajadoras  \n",
       "7257                          trabajadores  \n",
       "7258            funcionarios de la prisión  \n",
       "7259                            presidente  \n",
       "7260                                 preso  \n",
       "\n",
       "[7261 rows x 5 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = ds.head(n=None)[[\"tweet_id\", \"begin\", \"end\", \"type\", \"extraction\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"submissions/final_transformer_model/task7b_test.tsv\", sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
