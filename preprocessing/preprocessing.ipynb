{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with the transformers tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = tokenizer(\"check out this long sentence E.U. @testname\".split(), return_offsets_mapping=True, is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] check out this long sentence e. u. [UNK] testname [SEP]'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(indices[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [4, 4935, 2416, 12244, 14560, 12179, 2983, 8538, 1006, 1008, 1482, 1008, 3, 13444, 21613, 30955, 5], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 3), (3, 5), (0, 3), (0, 4), (0, 4), (0, 4), (4, 8), (0, 1), (1, 2), (2, 3), (3, 4), (0, 1), (1, 5), (5, 8), (8, 9), (0, 0)]}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'che',\n",
       " '##ck',\n",
       " 'out',\n",
       " 'this',\n",
       " 'long',\n",
       " 'sent',\n",
       " '##ence',\n",
       " 'e',\n",
       " '.',\n",
       " 'u',\n",
       " '.',\n",
       " '[UNK]',\n",
       " 'test',\n",
       " '##nam',\n",
       " '##e',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(indices[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.gold.biluo_tags_from_offsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacymoji\n",
    "#!pip install git+https://github.com/supadupa/spacymoji@fix-merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "import spacy\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from allennlp.data.token_indexers import PretrainedTransformerIndexer, PretrainedTransformerMismatchedIndexer, PretrainedTransformerIndexer\n",
    "from allennlp.data import Token, Vocabulary\n",
    "from biome.text.helpers import bioul_tags_to_bio_tags\n",
    "from spacymoji import Emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt = list(map(Path, sorted(glob.glob(\"../raw_data/subtask-2/brat/train/*.txt\"))))\n",
    "train_ann = list(map(Path, sorted(glob.glob(\"../raw_data/subtask-2/brat/train/*.ann\"))))\n",
    "\n",
    "valid_txt = list(map(Path, sorted(glob.glob(\"../raw_data/subtask-2/brat/valid//*.txt\"))))\n",
    "valid_ann = list(map(Path, sorted(glob.glob(\"../raw_data/subtask-2/brat/valid/*.ann\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "# emoji = Emoji(nlp, merge_spans=False)\n",
    "# nlp.add_pipe(emoji, first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_tokenizer(nlp):\n",
    "    # TODO: Not if this can be improved/generalized better\n",
    "    # yes, this is a skin tone ...\n",
    "    prefix_re = spacy.util.compile_prefix_regex(tuple(list(nlp.Defaults.prefixes) + ['[\\.\\üèΩ]']))\n",
    "    infix_re = spacy.util.compile_infix_regex(tuple(list(nlp.Defaults.infixes) + ['[\\-\\!\\.\\?\\\"\\(\\)\\:]']))\n",
    "    suffix_re = suffix_re = spacy.util.compile_suffix_regex(tuple(list(nlp.Defaults.suffixes) + []))   \n",
    "            \n",
    "    return Tokenizer(\n",
    "        nlp.vocab, \n",
    "        rules=nlp.tokenizer.rules,\n",
    "        prefix_search=prefix_re.search, \n",
    "        infix_finditer=infix_re.finditer,\n",
    "        suffix_search=suffix_re.search,\n",
    "        token_match=nlp.tokenizer.token_match,\n",
    "        url_match=nlp.tokenizer.url_match,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.tokenizer = create_custom_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000  ['']\n",
      "2000  ['']\n"
     ]
    }
   ],
   "source": [
    "def get_classification_dict(file_path: Path) -> Dict[str, str]:\n",
    "    \"\"\"A dict with {tweet_id: label}\"\"\"\n",
    "    classification_dict = {}\n",
    "    for i, line in enumerate(file_path.read_text().split('\\n')[1:]):\n",
    "        try:\n",
    "            tweet_id, label = line.split('\\t')\n",
    "            classification_dict[tweet_id] = label\n",
    "        except (ValueError, TypeError):\n",
    "            print(i, line, line.split('\\t'))\n",
    "    \n",
    "    return classification_dict\n",
    "\n",
    "train_classification = get_classification_dict(Path(\"../raw_data/subtask-1/train.tsv\"))\n",
    "valid_classification = get_classification_dict(Path(\"../raw_data/subtask-1/valid.tsv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(txt_files: List[Path], ann_files: List[Path], nlp: \"spacy.nlp\", classification: Dict[str, str]):\n",
    "    \n",
    "    data = {\n",
    "        \"raw_text\": [],\n",
    "        \"tokens\": [],\n",
    "        \"tags_bioul\": [],\n",
    "        \"tags_bio\": [],\n",
    "        \"entity_text\": [],\n",
    "        \"classification_label\": [],\n",
    "        \"file_name\": [],\n",
    "    }\n",
    "    \n",
    "    for txt, ann in tqdm(zip(txt_files, ann_files), total=len(txt_files)):\n",
    "        try:\n",
    "            doc: spacy.docs.Doc = helper.brat2doc(\n",
    "                txt, \n",
    "                ann, \n",
    "                nlp, \n",
    "                # The competition will only evaluate PROFESION and SITUACION_LABORAL\n",
    "                ignore_labels=[\"ACTIVIDAD\", \"FIGURATIVA\"],\n",
    "                remove_children=True,\n",
    "                remove_parents=False, \n",
    "                remove_siblings=False,\n",
    "                verbose=True,\n",
    "            )\n",
    "        except ValueError as error:\n",
    "            print(txt, ann)\n",
    "            raise error\n",
    "            \n",
    "        data[\"raw_text\"].append(doc.text)\n",
    "        data[\"tokens\"].append(list(map(str, doc)))\n",
    "        data[\"tags_bioul\"].append([token._.ctag for token in doc])\n",
    "        data[\"tags_bio\"].append(bioul_tags_to_bio_tags(data[\"tags_bioul\"][-1]))\n",
    "        data[\"entity_text\"].append(doc._.entity_text)\n",
    "        data[\"classification_label\"].append(classification[txt.name.split('.')[0]])\n",
    "        data[\"file_name\"].append(txt.name)\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd1faa3464e4e61a0f63ce43d1083e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Span(text='Protecci√≥n Civil', label='ACTIVIDAD', file='1244001571257581568.ann')\n",
      "Removed Span(text='JUGADORES', label='ACTIVIDAD', file='1244576816133791745.ann')\n",
      "Removed Span(text='jugadores', label='ACTIVIDAD', file='1244576816133791745.ann')\n",
      "Removed Span(text='actor', label='ACTIVIDAD', file='1245657701423525888.ann')\n",
      "Removed Span(text='cantante', label='ACTIVIDAD', file='1245657701423525888.ann')\n",
      "Removed Span(text='futbolista', label='ACTIVIDAD', file='1245657701423525888.ann')\n",
      "Removed Span(text='reina', label='FIGURATIVA', file='1246395424220549120.ann')\n",
      "Removed Span(text='maestro', label='FIGURATIVA', file='1250067856190066691.ann')\n",
      "Removed Span(text='jugadores', label='ACTIVIDAD', file='1250067856190066691.ann')\n",
      "Removed Span(text='jugadores', label='ACTIVIDAD', file='1250393594046885888.ann')\n",
      "Removed Span(text='deportistas', label='ACTIVIDAD', file='1251075915943219200.ann')\n",
      "Removed Span(text='ViceKomisario', label='FIGURATIVA', file='1252685852796432384.ann')\n",
      "Removed Span(text='Amado L√≠der Indecente', label='FIGURATIVA', file='1252685852796432384.ann')\n",
      "Removed Span(text='twitter@s', label='ACTIVIDAD', file='1252835132983005189.ann')\n",
      "Removed Span(text='#emprendedora', label='ACTIVIDAD', file='1254840432359587840.ann')\n",
      "Removed Span(text='#emprendedoras', label='ACTIVIDAD', file='1254840432359587840.ann')\n",
      "Removed Span(text='artistas', label='ACTIVIDAD', file='1254852827559555072.ann')\n",
      "Removed Span(text='Intelectuales', label='ACTIVIDAD', file='1254852827559555072.ann')\n",
      "Removed Span(text='voluntari@s', label='ACTIVIDAD', file='1256329335734075392.ann')\n",
      "Removed Span(text='voluntarios', label='ACTIVIDAD', file='1256653622202503172.ann')\n",
      "Removed Span(text='Autora', label='FIGURATIVA', file='1256883991648616449.ann')\n",
      "Removed Span(text='activista', label='ACTIVIDAD', file='1257201616790446083.ann')\n",
      "Removed Span(text='voluntarixs', label='ACTIVIDAD', file='1259121660868857856.ann')\n",
      "Removed Span(text='monologuista', label='FIGURATIVA', file='1259808033133207553.ann')\n",
      "Removed Span(text='instagrammer', label='FIGURATIVA', file='1259808033133207553.ann')\n",
      "Removed Span(text='artistazo', label='ACTIVIDAD', file='1260251958310580231.ann')\n",
      "Removed Span(text='voluntarios', label='ACTIVIDAD', file='1260303443157286915.ann')\n",
      "Removed Span(text='voluntarios', label='ACTIVIDAD', file='1260458967962173446.ann')\n",
      "Removed Span(text='mayorales', label='FIGURATIVA', file='1261319370011262984.ann')\n",
      "Removed Span(text='artistas', label='ACTIVIDAD', file='1263524316618031105.ann')\n",
      "Removed Span(text='doctorcillo', label='FIGURATIVA', file='1263548815556182021.ann')\n",
      "Removed Span(text='corredores', label='ACTIVIDAD', file='1263766219330433026.ann')\n",
      "Removed Span(text='voluntarios', label='ACTIVIDAD', file='1263949601628925952.ann')\n",
      "Removed Span(text='consejeros', label='FIGURATIVA', file='1264853803553128450.ann')\n",
      "Removed Span(text='Guardia Civil', label='PROFESION', file='1264885356626948097.ann')\n",
      "Removed Span(text='GC', label='PROFESION', file='1265000720018870274.ann')\n",
      "Removed Span(text='guardia civil', label='PROFESION', file='1265294111688601601.ann')\n",
      "Removed Span(text='ciclistas', label='ACTIVIDAD', file='1265387509237911552.ann')\n",
      "Removed Span(text='terroristas', label='ACTIVIDAD', file='1265743912066564097.ann')\n",
      "Removed Span(text='ministro del Tiempo', label='FIGURATIVA', file='1268807584636641280.ann')\n",
      "Removed Span(text='Guardia Civil', label='PROFESION', file='1269241724645318659.ann')\n",
      "Removed Span(text='cantantes', label='ACTIVIDAD', file='1270462460496093186.ann')\n",
      "Removed Span(text='nazareno', label='ACTIVIDAD', file='1270793931207454720.ann')\n",
      "Removed Span(text='REY', label='FIGURATIVA', file='1272107543607869440.ann')\n",
      "Removed Span(text='pseudo periodistas', label='FIGURATIVA', file='1272113182937669633.ann')\n",
      "Removed Span(text='jugadores', label='ACTIVIDAD', file='1272131597370830849.ann')\n",
      "Removed Span(text='ayudantes', label='ACTIVIDAD', file='1272131597370830849.ann')\n",
      "Removed Span(text='ARTISTA', label='ACTIVIDAD', file='1272828246380490752.ann')\n",
      "Removed Span(text='cantante', label='ACTIVIDAD', file='1273361796183855111.ann')\n",
      "Removed Span(text='personas voluntarias', label='ACTIVIDAD', file='1274978290571362309.ann')\n",
      "Removed Span(text='ide√≥logo', label='ACTIVIDAD', file='1275000691581497345.ann')\n",
      "Removed Span(text='voluntarios', label='ACTIVIDAD', file='1275717386247770112.ann')\n",
      "Removed Span(text='voluntarios', label='ACTIVIDAD', file='1276855332984090626.ann')\n",
      "Removed Span(text='buf√≥n', label='FIGURATIVA', file='1278610986228944897.ann')\n",
      "Removed Span(text='pedagogo', label='FIGURATIVA', file='1279658681731297280.ann')\n",
      "Removed Span(text='vocero', label='ACTIVIDAD', file='1280608237348425728.ann')\n",
      "Removed Span(text='vocera', label='ACTIVIDAD', file='1280608237348425728.ann')\n",
      "Removed Span(text='profetas', label='ACTIVIDAD', file='1281924340536287240.ann')\n",
      "Removed Span(text='miembros de las mesas', label='ACTIVIDAD', file='1282247806925840394.ann')\n",
      "Removed Span(text='reina', label='FIGURATIVA', file='1286291372627066881.ann')\n",
      "Removed Span(text='fil√°ntropos', label='ACTIVIDAD', file='1286439414785335299.ann')\n",
      "Removed Span(text='Voluntarios', label='ACTIVIDAD', file='1291102924010074112.ann')\n",
      "Removed Span(text='voluntarios', label='ACTIVIDAD', file='1291390764912717825.ann')\n",
      "Removed Span(text='voluntarios', label='ACTIVIDAD', file='1291390764912717825.ann')\n",
      "Removed Span(text='voluntarios', label='ACTIVIDAD', file='1293300892465324034.ann')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train = create_df(train_txt, train_ann, nlp, train_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65cb2e0008d04c179f647f9930a2bd67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Span(text='ama de casa', label='ACTIVIDAD', file='1247422026375200769.ann')\n",
      "Removed Span(text='atleta', label='FIGURATIVA', file='1252603036524052481.ann')\n",
      "Removed Span(text='autor', label='ACTIVIDAD', file='1253703655359987712.ann')\n",
      "Removed Span(text='necropol√≠ticos', label='FIGURATIVA', file='1255419044657758211.ann')\n",
      "Removed Span(text='necr√≥fagos pol√≠ticos', label='FIGURATIVA', file='1255419044657758211.ann')\n",
      "Removed Span(text='presidente', label='FIGURATIVA', file='1257030225277464584.ann')\n",
      "Removed Span(text='deportistas', label='ACTIVIDAD', file='1257601158065131522.ann')\n",
      "Removed Span(text='Voluntariado de Refuerzo educativo', label='ACTIVIDAD', file='1257783753277091842.ann')\n",
      "Removed Span(text='agentes', label='FIGURATIVA', file='1258327578475102209.ann')\n",
      "Removed Span(text='artistas de espect√°culos p√∫blicos', label='ACTIVIDAD', file='1261015959206232072.ann')\n",
      "Removed Span(text='deportistas', label='ACTIVIDAD', file='1263780155673149442.ann')\n",
      "Removed Span(text='Autor', label='ACTIVIDAD', file='1263905083965792258.ann')\n",
      "Removed Span(text='l√≠deres comunales', label='ACTIVIDAD', file='1266195217000140800.ann')\n",
      "Removed Span(text='hombres de trono', label='ACTIVIDAD', file='1270402085247430656.ann')\n",
      "Removed Span(text='cofrades', label='ACTIVIDAD', file='1270402085247430656.ann')\n",
      "Removed Span(text='voluntarios', label='ACTIVIDAD', file='1272166905655234565.ann')\n",
      "Removed Span(text='#ProteccionCivil', label='ACTIVIDAD', file='1272166905655234565.ann')\n",
      "Removed Span(text='tuitero', label='ACTIVIDAD', file='1272489573910818816.ann')\n",
      "Removed Span(text='due√±o', label='FIGURATIVA', file='1272668645135659009.ann')\n",
      "Removed Span(text='voluntarios', label='ACTIVIDAD', file='1276105057419907073.ann')\n",
      "Removed Span(text='payasa', label='FIGURATIVA', file='1276434801206190085.ann')\n",
      "Removed Span(text='artistas', label='ACTIVIDAD', file='1291317679803052033.ann')\n",
      "Removed Span(text='Rey em√©rito', label='ACTIVIDAD', file='1291422649290240002.ann')\n",
      "Removed Span(text='virreyes auton√≥micos', label='FIGURATIVA', file='1291745879129554950.ann')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_valid = create_df(valid_txt, valid_ann, nlp, valid_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cat data/profner/subtask-2/brat/train/1269241724645318659.ann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for misalignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'B-PROFESION',\n",
       "  'B-SITUACION_LABORAL',\n",
       "  'I-PROFESION',\n",
       "  'I-SITUACION_LABORAL',\n",
       "  'L-PROFESION',\n",
       "  'L-SITUACION_LABORAL',\n",
       "  'O',\n",
       "  'U-PROFESION',\n",
       "  'U-SITUACION_LABORAL'},\n",
       " {'B-PROFESION',\n",
       "  'B-SITUACION_LABORAL',\n",
       "  'I-PROFESION',\n",
       "  'I-SITUACION_LABORAL',\n",
       "  'O'},\n",
       " {'B-PROFESION',\n",
       "  'B-SITUACION_LABORAL',\n",
       "  'I-PROFESION',\n",
       "  'L-PROFESION',\n",
       "  'L-SITUACION_LABORAL',\n",
       "  'O',\n",
       "  'U-PROFESION',\n",
       "  'U-SITUACION_LABORAL'},\n",
       " {'B-PROFESION',\n",
       "  'B-SITUACION_LABORAL',\n",
       "  'I-PROFESION',\n",
       "  'I-SITUACION_LABORAL',\n",
       "  'O'})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_train.tags_bioul.sum()), set(df_train.tags_bio.sum()), set(df_valid.tags_bioul.sum()), set(df_valid.tags_bio.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df_valid.itertuples():\n",
    "    if '-' in row.tags_bioul:\n",
    "        print(row.file_name)\n",
    "        print(list(zip(row.tokens, row.tags_bioul)), row.raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags_bioul</th>\n",
       "      <th>tags_bio</th>\n",
       "      <th>entity_text</th>\n",
       "      <th>classification_label</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cerramos nuestra querida Radio üò¢ Nuestros cola...</td>\n",
       "      <td>[Cerramos, nuestra, querida, Radio, üò¢, Nuestro...</td>\n",
       "      <td>[O, O, O, O, O, O, U-PROFESION, O, U-PROFESION...</td>\n",
       "      <td>[O, O, O, O, O, O, B-PROFESION, O, B-PROFESION...</td>\n",
       "      <td>[colaboradores, conductores]</td>\n",
       "      <td>1</td>\n",
       "      <td>1242399976644325376.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#OtroEscandalo #HastaCuando \\n#DenunciaCCOO #C...</td>\n",
       "      <td>[#, OtroEscandalo, #, HastaCuando, \\n, #, Denu...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1242406334802395137.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>¬øEs necesario entregar nuestra privacidad a un...</td>\n",
       "      <td>[¬ø, Es, necesario, entregar, nuestra, privacid...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1242407077278093313.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As√≠ que est√°s chimbeando mucho con esos Decret...</td>\n",
       "      <td>[As√≠, que, est√°s, chimbeando, mucho, con, esos...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, U-PROFESION,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-PROFESION,...</td>\n",
       "      <td>[Presidente]</td>\n",
       "      <td>1</td>\n",
       "      <td>1242407274771030016.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@FeGarPe79 @escipion_r @LuciaMendezEM Est√°s MU...</td>\n",
       "      <td>[@FeGarPe79, @escipion_r, @LuciaMendezEM, Est√°...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1242409866515435520.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>Se avecina un nuevo confinamiento q es una man...</td>\n",
       "      <td>[Se, avecina, un, nuevo, confinamiento, q, es,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1293639784397766656.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>As√≠ funciona Radar COVID, la app de rastreo de...</td>\n",
       "      <td>[As√≠, funciona, Radar, COVID, ,, la, app, de, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1293642161867632641.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>Se duplican los contagios por Coronavirus en M...</td>\n",
       "      <td>[Se, duplican, los, contagios, por, Coronaviru...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1293651264140726272.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>Corr√≠janme si me equivoco, pero somos el √∫nico...</td>\n",
       "      <td>[Corr√≠janme, si, me, equivoco, ,, pero, somos,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1293654036722442247.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>As√≠ ha colaborado el hombre sin saberlo con el...</td>\n",
       "      <td>[As√≠, ha, colaborado, el, hombre, sin, saberlo...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1293654313311637506.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               raw_text  \\\n",
       "0     Cerramos nuestra querida Radio üò¢ Nuestros cola...   \n",
       "1     #OtroEscandalo #HastaCuando \\n#DenunciaCCOO #C...   \n",
       "2     ¬øEs necesario entregar nuestra privacidad a un...   \n",
       "3     As√≠ que est√°s chimbeando mucho con esos Decret...   \n",
       "4     @FeGarPe79 @escipion_r @LuciaMendezEM Est√°s MU...   \n",
       "...                                                 ...   \n",
       "5995  Se avecina un nuevo confinamiento q es una man...   \n",
       "5996  As√≠ funciona Radar COVID, la app de rastreo de...   \n",
       "5997  Se duplican los contagios por Coronavirus en M...   \n",
       "5998  Corr√≠janme si me equivoco, pero somos el √∫nico...   \n",
       "5999  As√≠ ha colaborado el hombre sin saberlo con el...   \n",
       "\n",
       "                                                 tokens  \\\n",
       "0     [Cerramos, nuestra, querida, Radio, üò¢, Nuestro...   \n",
       "1     [#, OtroEscandalo, #, HastaCuando, \\n, #, Denu...   \n",
       "2     [¬ø, Es, necesario, entregar, nuestra, privacid...   \n",
       "3     [As√≠, que, est√°s, chimbeando, mucho, con, esos...   \n",
       "4     [@FeGarPe79, @escipion_r, @LuciaMendezEM, Est√°...   \n",
       "...                                                 ...   \n",
       "5995  [Se, avecina, un, nuevo, confinamiento, q, es,...   \n",
       "5996  [As√≠, funciona, Radar, COVID, ,, la, app, de, ...   \n",
       "5997  [Se, duplican, los, contagios, por, Coronaviru...   \n",
       "5998  [Corr√≠janme, si, me, equivoco, ,, pero, somos,...   \n",
       "5999  [As√≠, ha, colaborado, el, hombre, sin, saberlo...   \n",
       "\n",
       "                                             tags_bioul  \\\n",
       "0     [O, O, O, O, O, O, U-PROFESION, O, U-PROFESION...   \n",
       "1     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3     [O, O, O, O, O, O, O, O, O, O, O, U-PROFESION,...   \n",
       "4     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "...                                                 ...   \n",
       "5995  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "5996  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "5997  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "5998  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "5999  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                               tags_bio  \\\n",
       "0     [O, O, O, O, O, O, B-PROFESION, O, B-PROFESION...   \n",
       "1     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3     [O, O, O, O, O, O, O, O, O, O, O, B-PROFESION,...   \n",
       "4     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "...                                                 ...   \n",
       "5995  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "5996  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "5997  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "5998  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "5999  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                       entity_text classification_label  \\\n",
       "0     [colaboradores, conductores]                    1   \n",
       "1                               []                    0   \n",
       "2                               []                    0   \n",
       "3                     [Presidente]                    1   \n",
       "4                               []                    0   \n",
       "...                            ...                  ...   \n",
       "5995                            []                    0   \n",
       "5996                            []                    0   \n",
       "5997                            []                    0   \n",
       "5998                            []                    0   \n",
       "5999                            []                    0   \n",
       "\n",
       "                    file_name  \n",
       "0     1242399976644325376.txt  \n",
       "1     1242406334802395137.txt  \n",
       "2     1242407077278093313.txt  \n",
       "3     1242407274771030016.txt  \n",
       "4     1242409866515435520.txt  \n",
       "...                       ...  \n",
       "5995  1293639784397766656.txt  \n",
       "5996  1293642161867632641.txt  \n",
       "5997  1293651264140726272.txt  \n",
       "5998  1293654036722442247.txt  \n",
       "5999  1293654313311637506.txt  \n",
       "\n",
       "[6000 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags_bioul</th>\n",
       "      <th>tags_bio</th>\n",
       "      <th>entity_text</th>\n",
       "      <th>classification_label</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COMUNICADO POR CORONAVIRUS \\nEl Presidente Rus...</td>\n",
       "      <td>[COMUNICADO, POR, CORONAVIRUS, \\n, El, Preside...</td>\n",
       "      <td>[O, O, O, O, O, U-PROFESION, O, O, O, O, O, O,...</td>\n",
       "      <td>[O, O, O, O, O, B-PROFESION, O, O, O, O, O, O,...</td>\n",
       "      <td>[Presidente]</td>\n",
       "      <td>1</td>\n",
       "      <td>1242407018465579008.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‚ÄúLa falta de transparencia en asuntos de salud...</td>\n",
       "      <td>[‚Äú, La, falta, de, transparencia, en, asuntos,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1242486580222103554.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Las ense√±anzas del coronavirus |\\nPero @jcoscu...</td>\n",
       "      <td>[Las, ense√±anzas, del, coronavirus, |, \\n, Per...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1242506188555718656.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No me alegro de la muerte de nadie, ¬øpero es m...</td>\n",
       "      <td>[No, me, alegro, de, la, muerte, de, nadie, ,,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[guardias civiles]</td>\n",
       "      <td>1</td>\n",
       "      <td>1242686975943094273.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UNIDOS venceremos al Coronavirus #COVID19 http...</td>\n",
       "      <td>[UNIDOS, venceremos, al, Coronavirus, #, COVID...</td>\n",
       "      <td>[O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1242726918132301825.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>‚ÄîMe he bajado la app esa para detectar enfermo...</td>\n",
       "      <td>[‚Äî, Me, he, bajado, la, app, esa, para, detect...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1293545412066975744.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>Desde el 7 de marzo, y mascarilla en ristre, a...</td>\n",
       "      <td>[Desde, el, 7, de, marzo, ,, y, mascarilla, en...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1293561267601510402.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>Sigan para bingo en este 2020</td>\n",
       "      <td>[Sigan, para, bingo, en, este, 2020]</td>\n",
       "      <td>[O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1293579520000368640.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>üá™üá∏ | URGENTE - CORONAVIRUS: Espa√±a reporta 169...</td>\n",
       "      <td>[üá™, üá∏, |, URGENTE, -, CORONAVIRUS, :, Espa√±a, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1293598083545214980.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>La mascarilla, en d√≠as de calor, puede ser alg...</td>\n",
       "      <td>[La, mascarilla, ,, en, d√≠as, de, calor, ,, pu...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1293647654359117827.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               raw_text  \\\n",
       "0     COMUNICADO POR CORONAVIRUS \\nEl Presidente Rus...   \n",
       "1     ‚ÄúLa falta de transparencia en asuntos de salud...   \n",
       "2     Las ense√±anzas del coronavirus |\\nPero @jcoscu...   \n",
       "3     No me alegro de la muerte de nadie, ¬øpero es m...   \n",
       "4     UNIDOS venceremos al Coronavirus #COVID19 http...   \n",
       "...                                                 ...   \n",
       "1995  ‚ÄîMe he bajado la app esa para detectar enfermo...   \n",
       "1996  Desde el 7 de marzo, y mascarilla en ristre, a...   \n",
       "1997                      Sigan para bingo en este 2020   \n",
       "1998  üá™üá∏ | URGENTE - CORONAVIRUS: Espa√±a reporta 169...   \n",
       "1999  La mascarilla, en d√≠as de calor, puede ser alg...   \n",
       "\n",
       "                                                 tokens  \\\n",
       "0     [COMUNICADO, POR, CORONAVIRUS, \\n, El, Preside...   \n",
       "1     [‚Äú, La, falta, de, transparencia, en, asuntos,...   \n",
       "2     [Las, ense√±anzas, del, coronavirus, |, \\n, Per...   \n",
       "3     [No, me, alegro, de, la, muerte, de, nadie, ,,...   \n",
       "4     [UNIDOS, venceremos, al, Coronavirus, #, COVID...   \n",
       "...                                                 ...   \n",
       "1995  [‚Äî, Me, he, bajado, la, app, esa, para, detect...   \n",
       "1996  [Desde, el, 7, de, marzo, ,, y, mascarilla, en...   \n",
       "1997               [Sigan, para, bingo, en, este, 2020]   \n",
       "1998  [üá™, üá∏, |, URGENTE, -, CORONAVIRUS, :, Espa√±a, ...   \n",
       "1999  [La, mascarilla, ,, en, d√≠as, de, calor, ,, pu...   \n",
       "\n",
       "                                             tags_bioul  \\\n",
       "0     [O, O, O, O, O, U-PROFESION, O, O, O, O, O, O,...   \n",
       "1     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "4                                 [O, O, O, O, O, O, O]   \n",
       "...                                                 ...   \n",
       "1995  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1996  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1997                                 [O, O, O, O, O, O]   \n",
       "1998  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1999  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                               tags_bio         entity_text  \\\n",
       "0     [O, O, O, O, O, B-PROFESION, O, O, O, O, O, O,...        [Presidente]   \n",
       "1     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...                  []   \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...                  []   \n",
       "3     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  [guardias civiles]   \n",
       "4                                 [O, O, O, O, O, O, O]                  []   \n",
       "...                                                 ...                 ...   \n",
       "1995  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...                  []   \n",
       "1996  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...                  []   \n",
       "1997                                 [O, O, O, O, O, O]                  []   \n",
       "1998  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...                  []   \n",
       "1999  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...                  []   \n",
       "\n",
       "     classification_label                file_name  \n",
       "0                       1  1242407018465579008.txt  \n",
       "1                       0  1242486580222103554.txt  \n",
       "2                       0  1242506188555718656.txt  \n",
       "3                       1  1242686975943094273.txt  \n",
       "4                       0  1242726918132301825.txt  \n",
       "...                   ...                      ...  \n",
       "1995                    0  1293545412066975744.txt  \n",
       "1996                    0  1293561267601510402.txt  \n",
       "1997                    0  1293579520000368640.txt  \n",
       "1998                    0  1293598083545214980.txt  \n",
       "1999                    0  1293647654359117827.txt  \n",
       "\n",
       "[2000 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_json(\"train_v1.json\", lines=True, orient=\"records\")\n",
    "df_valid.to_json(\"valid_v1.json\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing with the mask concept\n",
    "Basically only tag the first transformer token of a spacy token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_offsets(df, model_name=\"dccuchile/bert-base-spanish-wwm-uncased\") -> Tuple[List, List]:\n",
    "    destroyed_tokens = []\n",
    "    destroyed_tags = []\n",
    "    offs = []\n",
    "    indexer = PretrainedTransformerIndexer(model_name=model_name)\n",
    "    vocab = Vocabulary()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name) \n",
    "    \n",
    "    for row in tqdm(df.itertuples(), total=len(df)):\n",
    "        indices = tokenizer(row.tokens, return_offsets_mapping=True, is_split_into_words=True)\n",
    "        token_groups = []\n",
    "        for input_id, offset in zip(indices[\"input_ids\"][1:-1], indices[\"offset_mapping\"][1:-1]):\n",
    "            if offset[0] == 0:\n",
    "                token_groups.append([tokenizer.convert_ids_to_tokens(input_id)])\n",
    "            else:\n",
    "                token_groups[-1].append(tokenizer.convert_ids_to_tokens(input_id))\n",
    "        print(list(zip(row.tokens, token_groups)))\n",
    "        tokens_str = tokenizer.convert_ids_to_tokens(indices[\"input_ids\"])\n",
    "        tokens = [Token(tok) for tok in tokens_str]\n",
    "        token_indexes = indexer.tokens_to_indices(tokens, vocabulary=vocab)\n",
    "        token_ids = token_indexes[\"token_ids\"]\n",
    "                    \n",
    "    return destroyed_tokens, destroyed_tags, offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'check_offsets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-bebc0478824e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcheck_offsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'check_offsets' is not defined"
     ]
    }
   ],
   "source": [
    "check_offsets(df[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_json(\"train_for_bert.json\", lines=True, orient=\"records\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
